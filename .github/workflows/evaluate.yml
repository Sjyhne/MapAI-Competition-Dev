# This is a basic workflow to help you get started with Actions

name: Evaluation Pipeline

# Controls when the workflow will run
on:
  # Triggers the workflow on push or pull request events but only for the "master" branch
  push:
    branches: [ "master" ]
  pull_request:
    branches: [ "master" ]

  # Allows you to run this workflow manually from the Actions tab
  workflow_dispatch:

# A workflow run is made up of one or more jobs that can run sequentially or in parallel
jobs:
  # This workflow contains a single job called "build"
  evaluate:
    env:
      ACCESS_TOKEN: ${{ secrets.HF_EVAL_ACCESS_TOKEN }}
    # The type of runner that the job will run on
    runs-on: ubuntu-latest

    # Steps represent a sequence of tasks that will be executed as part of the job
    steps:
      # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it
      - name: Checkout
        uses: actions/checkout@v3

      - name: Install ubuntu dependencies
        run: sudo apt-get update && sudo apt-get install -y python3-pip ffmpeg libsm6 libxext6

      - name: Install Pyenv
        run: curl https://pyenv.run | bash

      # Install python packages that are required.
      - name: Install Python Global Dependencies
        run: python3 -m pip install tomli pyenv-api

      # Runs a set of commands using the runners shell
      # If not field github.event.pull_request exists, then do first forloop
      - name: Evaluation
        run: |
          # Setup pyenv
          export PYENV_ROOT="$HOME/.pyenv"
          command -v pyenv >/dev/null || export PATH="$PYENV_ROOT/bin:$PATH"
          eval "$(pyenv init -)"
          
          # Variables
          datapercentage=0.01
          
          for team in team_*/ ; do
            # Set-up python environment
            pyenv_exec=$(python3 .github/scripts/version_extractor.py --config $team/pyproject.toml)
            eval "$pyenv_exec -m venv env"
            source env/bin/activate
            
            # Install team dependencies
            pip3 install $team
          
            # Enter team directory
            cd $team/src
          
          
            if [ -z ${{ github.event.release }} ]; then
              # Validation tests (on public data)
              python3 evaluate_task_1.py --data_percentage $datapercentage --dtype validation
              python3 evaluation.py --data_percentage $datapercentage --dtype validation --task 1
              python3 evaluate_task_2.py --data_percentage $datapercentage --dtype validation
              python3 evaluation.py --data_percentage $datapercentage --dtype validation --task 2
            else
              # Validation tests (on private data, competition submission)
              echo $ACCESS_TOKEN | huggingface-cli login
              python3 evaluate_task_1.py --data_percentage $datapercentage --dtype task1
              python3 evaluate_task_2.py --data_percentage $datapercentage --dtype task2
              python3 evaluation.py --data_percentage $datapercentage --dtype task1 --task 1
              python3 evaluation.py --data_percentage $datapercentage --dtype task2 --task 2
              git add results_task_*.json
            fi
            
            # Deactivate virtual env
            deactivate
          
            echo "Finished evaluation for $team."
            cd ../..

          done

      - name: Commit files
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
          git commit -m "Adding result files generated during evaluation" -a
      - name: Push changes
        uses: ad-m/github-push-action@master
        with:
          github_token: ${{ secrets.ADD_RESULTS_TOKEN }}
          branch: ${{ github.ref }}
